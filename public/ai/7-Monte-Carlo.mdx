# 학습 방법
다이나믹 프로그래밍 =! 강화학습이란걸 알아야함, 따로따로 발발전하다 나나중에 합친 사람이 대단한거지

프리딕션 == 애벌루에이션, 똑같은의미임

S0 -> A0: 여기서파이(S0) 수행 후-> R0

애피소드라는걸 다 알수 있단거는, 지금까지 아는대 한에선수익을 전부 알수있다

# First visitVS. Every visit
미로찾기에서 만약내가 왔던길을 다시 가거 다시 반복하는 x1 -> x2 -> x1 -> x2..

이런식으로 됄때 어느걸 선택할가에 대한 것

***

몬테 카를로는 수렴할때까지 하지 않않음, 그래서 의사코드에서 while 빼고 그냥 마노이 돌돌려도됌

환환경이 없기 때문문에 강화학습에선 행동 가치함수만 사용하게 됀다.

몬테카를로에선 우리가 아는 부분에서만 가능하고, 모르는애들한테는 모모름, 환경을 아예 모르기 때문에

모르는 애들들한테 어떤 정책? 행행동을 수행할지지에 대해선 모험과 착취가 있겠는데

입실론 Greedy라고더 괜찮은 방법이 있음

몬테카를로에선 현재의 정책이 과거의 정책보다 더 좋다는걸 알수 이어야한다.

입실론 그리디를 했을때, 과거의 정책볻 현재가 더 좋다는걸 증명, 알수 있을까?

결국 최적의 정책으 찾을수 있는지?

시시그마 파이((a'|s) -e/||A|)/1-e= 1

온폴리시 몬테카카를로의 문제는 e/|A| 확률로만 탐험을 하게 됀다, 한마디로 자기가 아는 조그만 지식 내에서 몬갈 하는데 결국 탐험이 부족함

Off Policy가 강화학습에서 대부분 사용됌, 근데 좀 느리긴 함

예를들어 과제기간 3일남았는데 Off policy 쓰면 과제 못함, 학습시간이 3일, 2일이면 On policy 써야 함
