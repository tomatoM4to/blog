# 동적계획법과 벨만 방정식
여기서 Q(s) = ...Q(s') 이때 s'는 다음 상태지만, s도 포함하는 상태다, s는 현재상태

현재의 가치상태 함수 값은 당장 받을거랑 그다음 상태로 같을때의 수익을 더한것임

***

# 최적 상태 가치 함수의 추정
DP 는 모델을 알고있고 자연스럽게 현재 상태의 최적의 행동도 알고 있다.

근데 순차적으로 가는거라 최적의 행동을 해도 진짜 최적의 결과가 나오진 않는다.

결국 모르는건 V*(s) or V*(s') 다.

계속 말하지만 V*는 이론적으로 존재하는거지, 실제 조재하는지 조차 못한다.

그래서 결국 순순차적으로 추정해야 한한다. 사실 추정하는건 불가가능하다.

### EX)
전부 계산해 줬다 (교수님이)

1. 액션은 2개, 가속을 밟을지, 미친듯이 밟을지

2. 그냥 엑셀을 밟을경우 노말로 들엉ㅈ만 reward 1임

3. 미친듯이 밟으면 reward는 2지만 과열을 갈수도 노말로 갈수도 있음(50, 50)

4. 과열된 상태에서 그냥 엘셀을 밟으면 50, 50 으로 노말을 가거나 유지함

5. 과열에서 오버앨셀 하면 터짐, reward 는 10임


terminated에서 가치함수는 무조건 0이다 <- 중요함

***

# 싱크 & 어싱크
그냥 어싱크 쓰셈 그게 더 좋음, 이유는 그냥 최최신걸 쓰니까.. 왜 그런진 설명 못함

***

# 단점: 비효율적
DP의 단점이기도 함

이전 설명에서 사실 v1 에서 최적을 찾았기 때문에 v2에 갈 필요도 없음, 더 정확해지긴 하지만

더이상 가치함수에 대한 변화는 없다는 거지, 근본적으로 우리가 원하는건 파이*(a) 임

바로 정책을 찾는거지 가지함수를 찾는게 아니기에 쓰잘데기 없는 계산을 많이함

이 예예에선 2개의 상태지만, 아아타리 블럭깨기만 하더라도 상태가 수억개임

그래서 많이 사용돼는 방식은 아니지만매우 간단함

두번째론 시그그마가 겹쳐져 있게 모든 상태? 를 다 계계산 해야함

프로그래밍 적으로 보면 2중포문을 제곱만 돌돌려야함

강화학습에선 속도도 매우 중요함, 게임 같은 경우는 괜찮은데 사람이랑 상호작용 하려면 비용이 매우 많이듬

DP는 풀면 최적 정책을 100% 보장해주지만 현실적으론 어려움


